## 子网络
### 子网络数目的估计

在设计每一个子网络的结构之前，确定整个模型由几个这样的子网络构成至关重要。如果子网络数目太少，比如就只有一个，就不能发挥出模型融合的优势。但如果子网络数目太多，则会导致整体模型复杂度过高，出了加重训练的困难之外，过拟合的问题也会加重。且会造成时间和计算力的浪费。

关于估计子网络的数目，除了尝试不同数目的组合之外（一般为3，4个，不应过多，如超过10个），还可以用一下的方式计算：

1. 先设计一个子网络，训练完成后得到它的正确率，记为p；
2. 假设n个子模型是相互独立的话，整体模型的正确率为1-(1-p)^n。
3. 我们希望最终模型的正确率能大于某个值q：1-(1-p)^n > q，就能得到模型的个数n。

如：测试的到的子模型正确率为p = 50%，希望最终能得到q=80%的正确率，那么当n＝3时，1-0.5^3 > 0.8。子模型个数在3个左右。

上面不等式成立的一个重要前提是n个子模型相互独立。事实上，因为这些子网络模型都遵行着一定的设计规律，是无法做到相互独立的。换句话说三个正确率为50%的模型组合起来最终得到的正确率并不能达到80%。但也给我们提供了一种模型数目估计的方法。

### 子网络模型结构设计
现在还没有出现一种通用且有效的卷积网络模型结构设计的准则，所以实际操作中需要尝试不同结构的模型，检验其分类效果。但从前人的工作中还是能总结出一些可循的规律，为此我们将在本实验中我们遵循的模型结构设计范式总结如下：

子网络结构如表1所示，这三个子网络最主要的区别在于卷积层的个数不同。卷积层就如同图像的滤波器，它的目的是从输入中得到有用的特征信息。子网络卷积层个数越多，通常就认为它学习出的特征能够捕获到更多的细节。

确定好设计范式之后，还需要确定一些可选的参数，包括：

1. 卷积和采样的次数，即N和M的大小；
2. 卷积核大小，卷积步长；
3. 采样窗口大小、采样步长，以及采样的方式（最大值或是平均值）
4. 确定激活函数的种类（ReLU, Sigmoid, Tanh）；
5. 正则化的方式 (L1, L2, Dropout)；
6. 全联接层的个数，即K的大小；
7. learning rate 和 momentum 的大小。

我们通过尝试不同模型的效果，以及参考一些成熟的模型结构，最终把子网络设计如下：



输入层：**原始图像分辨率为$H\times W$，原始数据通过零均值化后通过输入层传递到网络中。

**卷积层：**我们为所有子网络选择的卷积核大小都为$3\times 3$，它是能够捕捉空间信息的最小的卷积核大小。我们在图像四周加入1个像素的衬垫，并把卷积步长固定为1，这样卷积前后的输入输出图像能够保持分辨率不变。

**非线性层：**非线性层采用的是普通ReLU函数：$\sigma(x)=max(0,x)$。

**下采样层：**我们采用的采样窗口大小为$2\times 2$，步长固定为2的最大值采样，所以并没有采样窗口间并没有重叠。

**全连接层：**在整个网络的末端我们层叠了3个全连接层，只在最后一个全联接层dropout，dropout rate定为0.5。

**输出层：**我们选取Softmax Loss做为整个网络的目标函数。

至于学习率learning rate，我们尝试了不同的数值，包括0.01,0.05,0.001等，均可收敛。momentum取0.9。
