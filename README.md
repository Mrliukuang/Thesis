## 子网络结构
现在还没有出现一种通用且有效的卷积网络模型结构设计的准则，所以实际操作中需要尝试不同结构的模型，检验其分类效果。但从前人的工作中还是能总结出一些可循的规律，为此我们将在本实验中我们遵循的模型结构设计范式总结如下：

子网络结构如表1所示，这三个子网络最主要的区别在于卷积层的个数不同。卷积层就如同图像的滤波器，它的目的是从输入中得到有用的特征信息。子网络卷积层个数越多，通常就认为它学习出的特征能够捕获到更多的细节。

确定好设计范式之后，还需要确定一些可选的参数，包括：

1. 卷积和采样的次数，即N和M的大小；
2. 卷积核大小，卷积步长；
3. 采样窗口大小、采样步长，以及采样的方式（最大值或是平均值）
4. 确定激活函数的种类（ReLU, Sigmoid, Tanh）；
5. 正则化的方式 (L1, L2, Dropout)；
6. 全联接层的个数，即K的大小；
7. learning rate 和 momentum 的大小。

我们通过尝试不同模型的效果，以及参考一些成熟的模型结构，最终把子网络设计如下：



输入层：**原始图像分辨率为$H\times W$，原始数据通过零均值化后通过输入层传递到网络中。

**卷积层：**我们为所有子网络选择的卷积核大小都为$3\times 3$，它是能够捕捉空间信息的最小的卷积核大小。我们在图像四周加入1个像素的衬垫，并把卷积步长固定为1，这样卷积前后的输入输出图像能够保持分辨率不变。

**非线性层：**非线性层采用的是普通ReLU函数：$\sigma(x)=max(0,x)$。

**下采样层：**我们采用的采样窗口大小为$2\times 2$，步长固定为2的最大值采样，所以并没有采样窗口间并没有重叠。

**全连接层：**在整个网络的末端我们层叠了3个全连接层，只在最后一个全联接层dropout，dropout rate定为0.5。

**输出层：**我们选取Softmax Loss做为整个网络的目标函数。

至于学习率learning rate，我们尝试了不同的数值，包括0.01,0.05,0.001等，均可收敛。momentum取0.9。
