Chapter 2. 人工神经网络
早在1940年\cite{}，神经网络模型就开始被用于模拟人脑的工作原理。人脑由10^{11}​个神经元 (neuron) 组成，每个神经元包含树突和轴突两个部分与外界连接，通过它们神经元可以接受不同的输入，并产生相应的输出。无数个这样的神经元通过复杂的结构连接在一起，构成了地球上最高等的生物的大脑。

人脑最最重要的特点就是它能够学习、进化。神经网络模型通过模拟神经元工作的原理，希望能够得到一个可以学习进化的模型。

本章节主要介绍经典的人工神经网络模型，以及它的训练法方法。

感知机

早在1958年，Frank Rosenblatt就提出了模拟单个神经元工作原理的感知机 (perceptron) 模型。感知机是一种前馈的网络模型，它包含一个输入层，通过计算输入层的各个输入与其对应权重的加权和，以及一个非线性的激活函数来完成工作。

感知机参数为W​，它接受输入向量x​，输出的值y = f(W\cdot x + b)​。其中f​是激活函数 (activation funtion)，它的作用是把输出归一化到某个固定的范围。最常用的激活函数包括sigmoid函数 f(z) = 1/(1+e^{-z})​和tanh函数f(z) = tanh(z)​。

感知机通过逐步调整参数W​来具备学习的能力。学习的过程是一个简单的算法，如果训练集和参数调整得当，神经元就能从中学到样本的一般规律。感知机可以作为一个二元的线性分类器，它最大的缺陷是它不能处理线性不可分问题。

人工神经网络

人工神经网络 (Artificial Neural Network，缩写ANN) 又被称为多层感知机 (Multilayer Perceptron，缩写MLP)，是一种模仿大脑结构和功能的数学计算模型。除了输入层，其余每个节点都是一个带有非线性激活函数的神经元。两个节点通过一个带权重的边连接在一起，这些权重就想当与神经网络的记忆。

可以证明，具备足够节点数量的神经网络可以对任意复杂的连续函数或逻辑表达进行无限逼近，所以多层感知器是一个通用的函数逼近器。这也是神经网络强大建模能力的根源。神经网络节点越多，参数就越多，那它能够被塑造成任意函数的能力就越强，这种能力被称为神经网络的“容量”。现代的深度神经网络可以包含数以亿计的参数，以及10到20层的神经元\cite{最新的微软的有150层}。

一种常见的多层神经网络通常由三部分组成：

输入层：神经网络的第一层，原始数据通过输入层神经元进入到整个神经网络中；

隐藏层：输入层后的一层或多层神经元，隐藏层越多，神经网络的非线性就越显著，从而神经网络的强健性就越显著；

输出层：神经网络的最后一层，输出的是最终分类的结果。

梯度下降与反向传播算法

梯度下降

梯度下降是一个参数优化的算法，它常被用于神经网络训练中。

梯度下降的目标是找打一个合适的参数x​使得函数f(x)​取得局部极小值。如果f(x)​是一个单调函数，那么局部的极小值也是全局的最小值。并且如果函数f(x)​在x=a​处可微且有定义，那么函数f(x)​在a​点沿着梯度相反的方向 -\nabla f(a)​ 下降最快。这样就可以得到递推公式：

x_{n+1} = x_{n}-\gamma_n \cdot \nabla f(x_{n})​

其中​称为步长 (step)。因此可以得到：

f(x_{n+1}) < f(x_n) < f(x_{n-1}) < … < f(1) < f(0)​

这样就逐步收敛到​的极值。

在实际训练中关于步长​的选择需特别谨慎，步长过小训练缓慢，难以收敛，且可能在反向传递中梯度消失 (gradient vanish)；反之步长过大又会梯度爆炸 (gradient explosion)。通常的做法是选择尝试不同的步长 (0.1, 0.01, …)，然后选择一个使目标函数下降最快的。

对于训练集样本以百万计的应用，为了更新一步参数，就得在整个训练集上计算目标函数和梯度，计算起来是很困难且浪费的。一个常用的做法是不在整个训练集上计算，而在其一个批量的子集上进行操作。因为随机子集样本和总体样本是同分布的，所以可以这样选择。例如对于ImageNet Challenge\cite{}，训练样本高达120万张图片，而训练批量的大小通常选择为128或者256。子集越小，每步更新速度就越快，但随即性就越强，达到收敛的所需的迭代次数就越多。这种在训练集上取一个随机子集来训练的方式称为批量梯度下降 (Mini-batch Gradient Descent)。

batch大小等于1的批量梯度下降又被称为随机梯度下降 (Stochastic Gradient Descent, SGD) 或者在线梯度下降 (On-line Gradient Gescent)。这是批量梯度下降的一种极端情形，但实际中很少使用。因为现代的数学计算库对于向量计算的支持和优化使得其比循环操作更加高效，所以通常我们称的随机梯度下降实际上就是指的批量梯度下降。

参数更新

利用梯度下降，参数有多种更新的方式，这里介绍其中最常见的三种。

普通更新

最普通的参数更新形式为：

x=x-lr\cdot dx​

其中lr为步长，又称为学习率 (learning rate)。只要学习率足够小的话，就能保证目标函数持续的下降。

动量更新

另一种参数更新方式称为动量更新 (Momentum update)。对于深度网络，通常动量更新的收敛速率更快。我们可以把损失函数可以看成是一座山的形状，随机初始化的参数等同于在某处放置一个速度为0的粒子，优化的过程可以看成模拟这颗粒子慢慢滚下山脉。粒子具有的势能为​，粒子受力与势能的梯度有关​，而​，所以梯度与粒子的加速度成正比。

v=\mu \cdot v - lr \cdot dx​

x = x + v​

参数​被称为动量，通常被设置成一个定值 (如0.9)。这个参数使得速度​减小，进而减少了粒子的动能，使得粒子最终能够停在山底。

Nesterov加速动量更新

Nesterov动量 (Nesterov's Accelerated Momentum, NAG) 更新是一种近几年流行的参数更新方式。它仍然是基于动量，且它对于凸函数理论上有着更好的收敛保证，实际运用中也比普通的动量更新效果要好。

Nesterov动量的核心思想在于它计算的不是当前位置​的梯度，而是用一个未来估计的位置​的梯度来替代：

x_{ahead} = x+\mu \cdot v​

v = \mu \cdot v - lr \cdot dx_{ahead}​

x = x + v​

上面表述的三种方法是同时对所有参数进行更新。除此之外还有对每个参数单独进行更新的方法，例如Adagrad, Adadelta, RMSprop等，在此不一一叙述。

反向传播算法

80年代中期，反向传播算法 (Backpropagation algorithm, 缩写：BP算法) 的发现和流行直接引导了人工神经网络领域研究的第二次热潮。反向传播就是使用求导的链式法则来迭代的计算每一层的梯度成为可能

反向传播就是不断的使用求导的链式法则 (chain rule) 来计算表达式的梯度值。

例如函数​

可以分解成：​

\frac{\partial f}{\partial q} = z, \frac{\partial f}{\partial z} = q​

\frac{\partial q}{\partial x} = 1, \frac{\partial q}{\partial y} = 1​

\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \cdot \frac{\partial q}{\partial x}​

\frac{\partial f}{\partial y} = \frac{\partial f}{\partial q} \cdot \frac{\partial q}{\partial y}​

其中向量​代表着函数​在​处的梯度。

神经网络的节点将输入向量与自身参数进行的是点乘操作，第​层的输出​。那么：

\frac{\partial y}{\partial W} = x,\frac{\partial y}{\partial x} = W​

而第​层的输入​又作为第​层的输出，这样梯度通过反向传播，就能计算出每一层的偏导数。而得到参数的偏导数就能用梯度下降的方法进行优化，最终达到学习的目的。

实现反向传播算法的核心在于实现每层的两种计算路径：

正向路径：通过输入x和参数w，计算输出y；

反向路径：通过输入x和参数w，以及输出y的梯度dz/dy，计算参数的梯度dz/dw以及输入的梯度dz/dx。

过拟合与正则化

过拟合

当一个模型的“容量”强大到能够把训练数据中的“噪音”都拟合进去的时候，就发生了“过拟合” (overfitting)。也就是说参数越多、越复杂的模型越容易发生过拟合。过拟合一般可以识为违反“奥卡姆剃刀”原则，即使用了过多的参数。

过拟合的一个典型的标志是在训练集上可以取得超高的正确率，一个足够复杂的模型可以在训练集上完美拟合，正确率100%；但在验证集上表现的异常糟糕。也就是说过拟合的模型对于新数据的一般化能力是很差的。

在统计和机器学习中，为了避免过拟合现象，须要使用额外的技巧（如正则化、交叉验证等），而正则化是其中应用的最广泛的一种。

正则化

正则化通过在目标函数中引入新的信息来预防过拟合的问题，新的目标函数为：

f(W,x) = loss(W,x) + \lambda\cdot R(W)​

其中​为正则项；​为正则项的权重参数。新的目标函数引入了对参数项的惩罚项，这样在最小化的过程中就对过多的参数进行限制，有效的控制了模型的复杂度，从而达到控制过拟合的目的。

L2-Norm

L2-Norm是最常用的正则项表达形式，它在目标函数中对参数的平方和进行惩罚：

R(W) = \sum_k W_{k}^2​

L2-Norm对与那些数值比较突出的参数惩罚很大，所以训练中L2-Norm倾向于选择那些分散，且都是比较小的数值。

L1-Norm

另一种较为常用的正则项称为L1-Norm，计算的是所有参数的绝对值和：

R(W) = \sum_k|W_{k}|​

L1-Norm可以与L2-Norm联合在一起使用：

R(W)=\sum_k{\lambda_1\cdot |W_k|+\lambda_2\cdot W_k^2}​

L1-Norm在优化过程中倾向于使参数变的稀疏，大多数的参数都接近于零。一般认为L2-Norm相比L1-Norm对过拟合的惩罚力度更强。至于选择哪种正则化的表达式以及权重参数​该如何确定需要在实际操作中通过交叉验证 (cross validation) 来确定。

注意在神经元点乘操作中引入的偏置项​一般是不算在正则项参数​中，这是因为​的系数永远是常数​，而常数是没有最小化的必要的。但在实际操作用算与不算，相差甚微。

Dropout

Dropout是近年来非常流行的简单而有效的正则化技巧。本质上它也是通过减少模型的参数达到控制过拟合的目的。在训练时，dropout让每个神经元(激活或抑制)以​的概率保持其原来的状态，或者使其变为抑制状态 (即输出0)。

//TODO: dropout示意图
