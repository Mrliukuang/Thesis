# Chapter 3. 卷积网络

卷积网络是人工神经网络的一个变种模型，它从生物学中演化而来。20世纪60年代，Hubel和Wiesel在研究猫脑皮层中关于局部敏感和方向选择的神经元时发现其独特的网络结构，继而提出了卷积神经网络（Convolutional Neural Network，简称CNN）。卷积网络克服了传统前馈神经网络在高维度输入时模型参数过多、计算困难的缺点，通过参数共享的方式大大降低了模型的复杂度。

在模式识别领域，卷积网络避免了对数据的前期预处理，可以直接输入原始数据，因而得到了广泛的运用。卷积网络在语音、图像、自然语言处理等方面，比其它的机器学习模型有着更好的表现。

在本章将会介绍卷积网络的各种构成模块，如卷积层、采样层以及各种激活函数等。除此之外还会介绍卷积网络两种常用的的目标函数。

## 卷积网络各层结构

卷积网络由执行不同操作的神经元层组成，通常会包含数据输入层、卷积层、非线性激活层、池化采样层、全相连层、损失函数输出层等。其中只有卷积层和全相连层包含参数，其余诸层并不需要进行参数优化。这些不同的功能层叠在一起构成了整个卷积网络的架构。也是通过这些功能层，卷积网络能够把原始数据输入映射到其所对应的类别中。

### 卷积层

卷积层将输入图像跟卷积层的卷积核进行卷积操作，将输入图像转化成包含特定特征的图像。卷积层实质上进行的是一种滤波操作。卷积网络的卷积层由一组可以学习的滤波器组成，且每组滤波器尺寸都比较小。卷积网络正向传递时，这些滤波器就在输入图像上进行卷积，生成特定的“激活图” (activation map)。这些二维的激活图层叠在一起就构成了一组三维的输出。直觉上来说，卷积网络就是要学习出能够在特定类型的图像特征上能够激活的卷积核。

卷积层的操作可以被描述为包含一个非线性函数 (即激活函数) 的映射操作：

$$f: \mathbb{R}^{M\times N\times K} \rightarrow \mathbb{R}^{M' \times N' \times K'}, \qquad  x \mapsto  y.$$

#### 局部相连与参数共享

卷积网络的卷积层区别于传统神经网络的一大特点是卷积网络的卷积层是局部连通 (local connectivity)的。在我们处理像图像这种维度很高的输入时，全相连的神经网络因为参数过多，所以它是不可实际操作的。取而代之，卷积网络将神经元只连接到其输入层的一个局部的区域，我们称之为感觉野或感受域 (receptive filed)。感受域的深度跟输入层的深度要保持一致。如输入层是$[32 \times 32 \times 3]$，感受域大小是$[5 \times 5]$，这样卷积层的每个神经元与输入层$[5\times 5\times 3]$个神经元相连。只需要$5\times 5\times 3=75$个参数，大大降低了所需的参数数量。之所以卷积网络能够采取参数共享的策略是它做了一个合理的假设：如果某种特征在图像的$(x_1, y_1)$位置起到了作用，那么在一个不同的位置$(x_2, y_2)$，它应该同样有用。这样我们在$(x_1, y_1)$位置训练的到的滤波器在$(x_2, y_2)$位置不用重复训练也同样有用。反之，如果我们想在不同的位置上学习到不同的特征，就不能采用参数共享的方式，而应该使用全相连的方式。

#### 激活函数的选择

常用的激活函数包括：Sigmoid, Tanh, ReLU, Leaky ReLU, Maxout等。

**Sigmoid:** $\sigma(x) = 1 / (1 + e^{-x})$把实数输入压缩到[0,1]范围内。Sigmoid函数很好的模拟了神经元从完全静止(0)到完全激发(1)的状态，所以它曾经比较流行。但它有两个重大的缺陷：

1. Sigmoid函数的输出不是以零为均值的。这一点很不方便，因为卷积网络的参数都初始化成以零为均值。这种不一致会导致训练过程中的收敛困难。
2. Sigmoid函数在两端时梯度非常小，接近于零。当输入取值在两端时，卷积网络的参数并不能得到更新。

**Tanh:** $\sigma(x) = tanh(x)$把输入压缩到[-1,1]之间，它成功的克服了Sigmoid函数的第一个缺点，Tanh的输出是以零为均值的。所以理论上，Tanh要比Sigmoid效果好。但它在两端的梯度仍然接近零，仍然会“杀死”梯度。

**ReLU:** $\sigma(x)=max(0,x)$，修正线性单元 (The Rectified Linear Unit, ReLU) 最近几年变的非常流行。ReLU把函数在$x=0$处截断。它有如下优点：

1. 大大加速训练。根据论文\cite{Krizhevsky, imagenet}描述，ReLU比Sigmoid和Tanh收敛加速6倍。其原因可能是其线性但不饱和的表达形式。
2. 计算简单。ReLU对比Sigmoid和Tanh计算都要简单，直接在0点截断即可。

但ReLU也有它自己的问题。例如在训练中某个时刻$x\le 0$，自此时刻起其回传的梯度永远为0。所以ReLU在训练中极易（40%甚至一半以上的参数）引起“梯度死亡” 。通过调节learning rate，可以避免出现这类问题。

**Leaky ReLU:** Leaky ReLU尝试修复普通ReLU的“梯度死亡”问题。对于$x<0$，Leaky ReLU仍然保有一个小的坡度 (例如0.01)。$\sigma(x)=1(x<0)\cdot (\alpha x)+1(x\ge 0)\cdot x$，$\alpha$是个小的常量。

**Maxout:** Maxout计算$\sigma(x)=max(w_1^Tx+b_1,w_2^Tx+b_2)$。ReLU及Leaky ReLU都是其一般化形式。Maxout综合了两者的优点，且不会出现“梯度死亡”的问题。但它把参数的数量翻倍，加重了计算的负担。

在实际操作中，往往就是使用最简单的ReLU函数，通过小心调节learning rate，来避免“梯度死亡“的问题”。如果效果不好，就尝试使用Leaky ReLU或者Maxout。

### 池化采样层

参数越多，模型复杂度越高，也就更容易发生过拟合 (over-fitting) 的问题。池化采样层通过降低激活图的维度来简化模型，显著减少模型参数数量，一方面能够降低过拟合的风险，另一方面也能减少训练所需的运算上的代价。

采样操作让数据维度降低，使之更加可控，但又不丢失其过多的信息。最常使用的最大值采样 (max pooling) 通过只保留池化窗口$H\times W$内的最大值来完成下采样操作。以$2\times 2$大小的采样窗口为例，如果滑动窗口的跨度为$2$的话，采样后的到的结果只有原来的$25\%$，也就是说其余$75\%$的“不重要信息”被省略了，这样就大大降低了模型的复杂度。

除了最大值池化之外，平均值采样 (average pool) 也常被使用。它保留的是采样窗口内的平均值，而不是最大值。在实际使用中，优先使用最大值采样，如果效果不好，则尝试使用平均值采样。

### 全相连层

全相连层与传统神经网络一样，其中的每个神经元与之前一层和之后一层的所有神经元都相连。通过向量的点乘操作来完成操作。全相连层也可以看成是卷积核大小为$1\times 1$的卷积层。由若干全相连层构成的神经网络通常位于卷积网络的最末端。最后一个全相连层的结果会输入到一个损失函数中来训练整个网络模型。

## 损失函数

损失函数 (Loss function) 又常被称为目标函数 (Objective function) ，用于衡量在训练集上训练结果的好坏程度。如果损失函数的结果大，说明分类效果很差；损失函数输出的结果小，就说明分类效果理想。这样，通过损失函数就能把卷积网络的训练转化成一个优化问题：如何获得合适的参数使损失函数的输出结果最小？

SVM Loss和Softmax Loss是卷积网络（以及神经网络）中最常使用的两种损失函数。

### SVM Loss

对于第$i$个训练样本$x_i$同它对应的类别$y_i$，卷积网络的到的最终得分由$f(x_i, W)$表示。样本$(x_i,y_i)$的SVMLoss为：

$$L_i = \sum_{j\neq y_i} \max(0, f(x_i, W)_j - f(x_i, W)_{y_i} + \Delta)$$

由SVMLoss的表达式可知正确类别$y_i$对应的得分$f(x_i,W)_{y_i}$比错误的类别$j$的得分$f(x_i,W)_j$至少大$\Delta$时，损失才为$0$。这种$max(0,-)$在$0$点处截断的损失函数又被称为hinge loss。它另一种常见的变形是它的平方形式$max(0,-)^2$，被称为L2SVM。L2SVM对于需要被惩罚的点惩罚的程度更严重。虽然没有平方的形式更加普遍一些，但在有些数据集上L2SVM的表现更好，需要通过交叉验证的方式进行选择。

### Softmax Loss

另一种常见的目标函数是Softmax Loss函数，它是二分逻辑回归分类器的更为一般表达。

Softmax函数将实数向量$z$中的每个值归一化道$[0, 1]$之间：

$$f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$$

Softmax函数的到的$[0,1]$之间的结果可以解释为概率值，也就是给定了样本$x_i$和参数$W$，最终模型把$x_i$分到它正确的类别$y_i$的概率是：

$$P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }$$

Softmax Loss通过计算$-log(P)$来作为它的目标函数：

$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \  \text{or equivalently} \  L_i = -f_{y_i} + \log\sum_j e^{f_j}$$

SVM Loss和Softmax Loss是两种最常被使用的目标函数。Softmax Loss通过最大化每个类别正确的“概率”来训练模型，理解起来可能更加直观一些，但实际使用中两者效果相当。

## 卷积网络的训练

### 数据预处理

常用的数据预处理方式包括零均值化、归一化，主成分分析和数据白化。

#### 零均值化

零均值化 (zero mean/mean subtraction) 是最常见的数据预处理方式。对于数据集$X$中的每个样本$x_i$，都减去所有样本的均值$\bar{X}$：

$X' = X- \bar{X}$

#### 归一化

归一化 (normalization) 将数据的维度伸缩到一个大致统一的范围内。有两种常见的标准化方式：

1. 数据的每个维度除以它的标准差：$X = \frac{X}{std(X)}$
2. 等比放缩，使每维数据的最大值最小值分别是$1$与$－－1$。

#### 主成分分析和数据白化

主成分分析 (Principal Component Analysis, 缩写：PCA) 是最常用的降维算法。它通过对协方差矩阵进行特征分解，以得出数据的主成分（即特征向量）与它们的权值（即特征值）。然后通过保留对方差影响最大的特征向量，而忽略影响小的。如果一个特征份量它对应的特征值越大，可以理解成这个分量包含的数据整体分布的特征信息越多，也就越重要。

白化操作把数据的每个维度都除以对应的特征值，这样使每个维度的分布趋于平均。

//TODO: 一张图

在实际操作中，零均值化和归一化是必不可少的，但如果数据本身就在一个确定的范围内，如图像像素就在[0,255]之间，归一化也可省略。主成分分析和数据白化需要根据具体的问题，适当选择。

### 参数初始化

卷积网络的训练过程就是参数的优化过程，参数的初始化必不可少。

一个常见的误区是把所有参数初始化成0。如果卷积网络的所有参数都是0，那么正向每个神经元的输出都是0，反向所有回传的梯度也都是0，参数根本就不会被更新，整个网络没有在学习！

正确的做法是把参数初始化成随机的小数值。所有参数必须非常接近0，但又不能等于0。常见的做法是参数从一个标准高斯分布($\mu = 0,\sigma = 1$)或者均匀分布上取值，再乘以一个很小的系数$\alpha$例如(0.001)。

但值得注意的是，参数初始化成小数值并不一定意味着就越好。因为回传的梯度和参数成正比，参数越小，梯度就越小。对于一个很深的网络来说，梯度在回传时会层层减小，就很可能会出现梯度消失的问题。 
